receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  # memory_limiter
  # https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md#best-practices
  # best practices
  memory_limiter:
    check_interval: 1s
    limit_percentage: 80
    spike_limit_percentage: 20

  # groupbytrace
  # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/groupbytraceprocessor/README.md
  # 
  groupbytrace:
    wait_duration: 10s
    num_traces: 100000
  
  # batch
  # https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/batchprocessor
  # default settings
  batch:
    send_batch_size: 8192
    timeout: 200ms

exporters:
  otlp:
    endpoint: jaeger:4317
    tls:
      insecure: true
    # sending_queue
    # https://github.com/open-telemetry/opentelemetry-collector/blob/main/exporter/exporterhelper/README.md#sending-queue
    # default settings
    sending_queue:
      num_consumers: 10
      queue_size: 1000
    retry_on_failure:
      enabled: true

  prometheus:
    endpoint: 0.0.0.0:9090
    namespace: testapp

  debug:
    verbosity: basic

service:
  telemetry:
    metrics:
      readers:
        - pull:
            exporter:
              prometheus:
                host: '0.0.0.0'
                port: 8888
  extensions: []
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, groupbytrace, batch]
      exporters: [otlp, debug]
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [prometheus, debug]
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlp, debug]
