# OTel Collector メモリ高騰シナリオと診断ガイド

実務で遭遇しやすく、かつ Collector の安定稼働を脅かす典型的なメモリ高騰シナリオに絞り、その際の特徴（シグネチャ）を理解するためのガイド。

---

## クイックリファレンス

| 番号 | シナリオ | コマンド | 主な観察ポイント |
|------|---------|---------|-----------------|
| **1** | **下流停止** | `make scenario-1` | Queue Usage 100%, Receiver Refused |
| **2** | **キャパシティ不足** | `make scenario-2` | Queue 上下動, CPU 張り付き, 慢性的な Refused |
| **3a** | **ステートフルprocessor（正常系）** | `make scenario-3a` | Heapが一定レベルで安定 |
| **3b** | **ステートフルprocessor（高カーディナリティ）** | `make scenario-3b` | Heapが右肩上がり、GC後も戻らない |

---

## シナリオ1: 下流（バックエンド）の遅延・停止

**現象**:
Jaeger や Prometheus などのバックエンドが応答しない、または極端に遅い場合。Collector はデータをメモリ上のキューに溜め込み、メモリ使用量が急増する。

**再現手順**:
1. `make scenario-1` で負荷開始。
2. `docker compose stop jaeger` でバックエンドを停止。

**診断のシグネチャ (Grafana)**:
- **Queue Usage**: 0% から一直線に **100%** に張り付く。
- **Heap Memory**: Queue の蓄積に伴い急増。リミット到達時に **Force GC による急落と高止まり** が発生。
- **Receiver Refused**: Queue が満杯になると `otelcol_receiver_refused_spans_total` が増加し、データ欠損が始まる。

**対処法**:
- `sending_queue` のサイズを適正化（大きくしすぎない）。
- `memory_limiter` を設定し、プロセス全体のクラッシュ（OOM Kill）を防ぐ。

---

## シナリオ2: 慢性的な入力過多（キャパシティ不足）

**現象**:
Collector の処理能力（CPU/Network）の限界を超えるデータが常に流入している状態。バックエンドは生きているが、処理が追いつかずメモリが上限付近で推移する。

**再現手順**:
1. `make scenario-2` を実行。

**診断のシグネチャ (Grafana)**:
- **Queue Usage**: 100% に張り付くのではなく、**高位（70-90%以上）で激しく上下動** する。
- **CPU Usage**: 常に高い使用率。
- **Receiver Refused**: 停止していないにもかかわらず、恒常的に `refused` が発生する。

**対処法**:
- Collector の水平スケール（台数を増やす）。
- 不要な属性の削除やサンプリングの導入。

---

## シナリオ3: ステートフルprocessorによるメモリ問題

`groupbyattrs` などのステートフルprocessorは、内部にマップを保持してデータを蓄積する。入力データの特性（カーディナリティ）によって、正常動作と異常動作が分かれる。

### 実務での発生パターン

1. 開発環境で `groupbyattrs` を設定、動作確認OK
2. 本番投入、最初は問題なし
3. 新機能リリースでログに `request_id` を追加
4. **突然メモリ使用量が爆発、OOM Kill**

原因: 開発環境では同じユーザーが繰り返しアクセス（低カーディナリティ）、本番では毎回異なるrequest_id（高カーディナリティ）

---

### シナリオ3a: 正常系（ベースライン）

**目的**: ステートフルprocessorの正常動作を確認。属性値の種類が限られているため、マップサイズに上限がある。

**再現手順**:
1. `make scenario-3a` を実行し、5分間観察。

**診断のシグネチャ (Grafana)**:
- **Heap Memory**: 初期蓄積後、一定レベルで安定。
- **GC の挙動**: 正常な上下動（蓄積→GC→解放の繰り返し）。

---

### シナリオ3b: 異常系（高カーディナリティ爆発）

**目的**: 毎回ユニークな属性値（UUID等）によるマップの無限膨張を観察。

**再現手順**:
1. `make scenario-3b` を実行し、5分間観察。

**診断のシグネチャ (Grafana)**:
- **Heap Memory**: 時間とともに右肩上がりに増加。
- **GC 後の挙動**: GCが走ってもメモリが元の水準まで戻らない。
- **memory_limiter発火**: 最終的に `otelcol_receiver_refused_spans_total` が増加。

**3aと3bの比較ポイント**:
- Collector設定は同一（`groupbyattrs` の keys も同じ）
- 違いは `-high-cardinality` フラグのみ（loadgenの属性値がUUID付きになる）
- 高カーディナリティ属性の影響を定量的に観察できる

---

### 対処法

- グループ化キーから高カーディナリティ属性（ID、ハッシュ等）を除外。
- `memory_limiter` をステートフルprocessorの **前** に配置。
- 本番投入前に、実際のデータのカーディナリティを確認。

---

## 診断フローチャート（簡易版）

```
メモリ高騰を検知
       │
       ▼
┌──────────────────┐
│ Queue Usage は？ │─── 100% ──▶ シナリオ1 (下流停止)
└──────────────────┘
       │
      正常〜高位
       │
       ▼
┌──────────────────────────┐
│ Refused は恒常的？       │─── Yes ──▶ シナリオ2 (キャパ過多)
└──────────────────────────┘
       │
       No
       │
       ▼
┌──────────────────────────┐
│ Heap が右肩上がり？      │─── Yes ──▶ シナリオ3b (高カーディナリティ)
│ GC後も戻らない？         │
└──────────────────────────┘
       │
       No (安定)
       │
       ▼
正常動作 または シナリオ3a相当
```
