# OTel Collector メモリ高騰シナリオと診断ガイド

実務で遭遇しやすく、かつ Collector の安定稼働を脅かす典型的なメモリ高騰シナリオに絞り、その際の特徴（シグネチャ）を理解するためのガイド。

---

## クイックリファレンス

| 番号 | シナリオ | コマンド | 主な観察ポイント |
|------|---------|---------|-----------------|
| **1** | **下流停止** | `make scenario-1` | Queue Usage 100%, Receiver Refused |
| **2** | **キャパシティ不足** | `make scenario-2` | Queue 上下動, CPU 張り付き, 慢性的な Refused |
| **3a** | **ステートフルprocessor（正常系）** | `make scenario-3a` | Heapが一定レベルで安定 |
| **3b** | **ステートフルprocessor（高カーディナリティ）** | `make scenario-3b` | Heapが右肩上がり、GC後も戻らない |
| **4** | **batchバースト処理** | `make scenario-4` | メモリのテントウ虫型グラフ、スパイク時の挙動 |

---

## シナリオ1: 下流（バックエンド）の遅延・停止

**現象**:
Jaeger や Prometheus などのバックエンドが応答しない、または極端に遅い場合。Collector はデータをメモリ上のキューに溜め込み、メモリ使用量が急増する。

**再現手順**:
1. `make scenario-1` で負荷開始。
2. `docker compose stop jaeger` でバックエンドを停止。

**診断のシグネチャ (Grafana)**:
- **Queue Usage**: 0% から一直線に **100%** に張り付く。
- **Heap Memory**: Queue の蓄積に伴い急増。リミット到達時に **Force GC による急落と高止まり** が発生。
- **Receiver Refused**: Queue が満杯になると `otelcol_receiver_refused_spans_total` が増加し、データ欠損が始まる。

**対処法**:
- `sending_queue` のサイズを適正化（大きくしすぎない）。
- `memory_limiter` を設定し、プロセス全体のクラッシュ（OOM Kill）を防ぐ。

---

## シナリオ2: 慢性的な入力過多（キャパシティ不足）

**現象**:
Collector の処理能力（CPU/Network）の限界を超えるデータが常に流入している状態。バックエンドは生きているが、処理が追いつかずメモリが上限付近で推移する。

**再現手順**:
1. `make scenario-2` を実行。

**診断のシグネチャ (Grafana)**:
- **Queue Usage**: 100% に張り付くのではなく、**高位（70-90%以上）で激しく上下動** する。
- **CPU Usage**: 常に高い使用率。
- **Receiver Refused**: 停止していないにもかかわらず、恒常的に `refused` が発生する。

**対処法**:
- Collector の水平スケール（台数を増やす）。
- 不要な属性の削除やサンプリングの導入。

---

## シナリオ3: ステートフルprocessorによるメモリ問題

`groupbyattrs` などのステートフルprocessorは、内部にマップを保持してデータを蓄積する。入力データの特性（カーディナリティ）によって、正常動作と異常動作が分かれる。

### 実務での発生パターン

1. 開発環境で `groupbyattrs` を設定、動作確認OK
2. 本番投入、最初は問題なし
3. 新機能リリースでログに `request_id` を追加
4. **突然メモリ使用量が爆発、OOM Kill**

原因: 開発環境では同じユーザーが繰り返しアクセス（低カーディナリティ）、本番では毎回異なるrequest_id（高カーディナリティ）

---

### シナリオ3a: 正常系（ベースライン）

**目的**: ステートフルprocessorの正常動作を確認。属性値の種類が限られているため、マップサイズに上限がある。

**再現手順**:
1. `make scenario-3a` を実行し、5分間観察。

**診断のシグネチャ (Grafana)**:
- **Heap Memory**: 初期蓄積後、一定レベルで安定。
- **GC の挙動**: 正常な上下動（蓄積→GC→解放の繰り返し）。

---

### シナリオ3b: 異常系（高カーディナリティ爆発）

**目的**: 毎回ユニークな属性値（UUID等）によるマップの無限膨張を観察。

**再現手順**:
1. `make scenario-3b` を実行し、5分間観察。

**診断のシグネチャ (Grafana)**:
- **Heap Memory**: 時間とともに右肩上がりに増加。GC後も戻らない。
- **スループット**: 段階的に低下（5,500 → 4,400 → 2,800 → 150 spans/sec）
- **memory_limiter発火**: **60秒後**に初回発火、以降も継続的に発火。
- **Refused Spans**: memory_limiter発火後から増加開始。
- **最終的に OOM Kill**: Collector が512MB制限に到達してクラッシュ。

**定量的な指標（実測値）**:
- memory_limiter初回発火: **60秒後**
- 平均スループット: target 8,000 の **59.6%** (4,768 spans/sec)
- Collector状態: **OOM Kill** (295秒後)
- 下流（Jaeger）への影響: **6.07 GB** メモリ使用

**3aと3bの比較ポイント**:
- Collector設定は同一（`groupbyattrs` の keys も同じ）
- 違いは `-high-cardinality` フラグのみ（loadgenの属性値がUUID付きになる）
- 高カーディナリティ属性の影響を定量的に観察できる
- **重要**: 下流のバックエンド（Jaeger）にも影響が波及する

---

### 対処法

- グループ化キーから高カーディナリティ属性（ID、ハッシュ等）を除外。
- `memory_limiter` をステートフルprocessorの **前** に配置。
- 本番投入前に、実際のデータのカーディナリティを確認。

---

## シナリオ4: batchプロセッサの不適切な設定（バースト処理の失敗）

**現象**:
トラフィックスパイク時にメモリが急増し、スパイク後の復帰が遅い。スパイクが頻繁に発生すると、メモリが高止まりして Collector が不安定になる。

**再現手順**:
1. `make scenario-4` を実行し、3分間観察。

**シナリオ4の設定（極端に不適切な例）**:
```yaml
memory_limiter:
  limit_percentage: 40       # 低すぎ（データ欠損を発生させる）
  spike_limit_percentage: 10 # 低すぎ

batch:
  send_batch_size: 20000     # 極端に大きい
  send_batch_max_size: 30000 # 上限も極端に大きい
  timeout: 10s               # 極端に長い

sending_queue:
  queue_size: 10000          # 極端に大きい
```

**重要**: `memory_limiter` の閾値を **40%**（205 MB）に設定することで、スパイク時に **データ欠損** を発生させます。これにより、requirement.md の目的「閾値を超えるとデータ欠損が発生する」を再現します。

**診断のシグネチャ (Grafana)**:
- **Heap Memory**: 「テントウ虫型」グラフ（スパイク→復帰を10秒ごとに繰り返す）
  - スパイク時: 急増（200-300 MB）
  - **memory_limiter 発火**: 205 MB（40%）を超えると Force GC
  - 通常時: 徐々に減少（だが復帰が遅い）
- **Receiver Refused**: **スパイク時にデータ欠損が発生** ⚠️
  - memory_limiter 発火により、新規データを拒否
- **Queue Usage**: スパイク時に急増（30-50%）、通常時に減少
- **Batch Send Size**: スパイク時に 20,000 のバッチが生成される
- **スループット**: 通常時 1,500 spans/sec、スパイク時 3,900 spans/sec（実測）

**負荷パターン**:
```
10秒: 通常負荷（1,500 spans/sec） → Heap 50MB
10秒: スパイク（15,000 spans/sec）→ Heap 急増 150MB 📈
10秒: 通常負荷（1,500 spans/sec） → Heap 徐々に減少 100MB（復帰遅い）
10秒: スパイク（15,000 spans/sec）→ Heap 再び急増 180MB 📈
... 繰り返し（徐々に高止まり）
```

**根本原因**:
- **`send_batch_size: 20000`**: スパイク時に 20,000 個溜まるまで送信されない → 大量メモリ蓄積
- **`timeout: 10s`**: 通常時に 10秒待つ → スパイク後のメモリ解放が極端に遅い
- **`queue_size: 10000`**: 極端に大きい Queue により、さらにメモリ蓄積

**ベースライン設定との比較**:
| 設定 | ベースライン（適切） | シナリオ4（極端に不適切） | 影響 |
|------|---------------------|---------------------------|------|
| `limit_percentage` | 80% (410 MB) | **40%** (205 MB) | データ欠損を発生させる |
| `send_batch_size` | 2,000 | **20,000** (10倍) | スパイク時の大量メモリ蓄積 |
| `timeout` | 200ms | **10s** (50倍) | 復帰速度が極端に遅い |
| `queue_size` | 500 | **10,000** (20倍) | さらに大量メモリ蓄積 |

**対処法**:
- **`send_batch_size` を小さく**: 2,000 程度に戻す
- **`timeout` を短く**: 200ms 程度に戻す
- **`queue_size` を小さく**: 500 程度に戻す
- 「スループットを上げようとして設定を盛りすぎると、スパイク耐性が下がる」

**重要な考察**:
このシナリオは「設定を大きくすればスループットが上がる」という誤解を検証する。
実際には、大きすぎる設定はスパイク耐性を下げ、メモリ高騰を招く。

---

## 診断フローチャート

```
メモリ高騰を検知
       │
       ▼
┌────────────────────────┐
│ メモリの変化パターンは？│
└────────────────────────┘
       │
       ├─── テントウ虫型（周期的上下動） ──▶ ┌────────────────────────┐
       │                                     │ 復帰速度は？           │
       │                                     └────────────────────────┘
       │                                            │
       │                                            ├─── 遅い（高止まり） ──▶ シナリオ4 (不適切な batch 設定)
       │                                            │
       │                                            └─── 速い ──▶ 正常動作（適切な設定）
       │
       ├─── 右肩上がり（GC後も戻らない）──▶ シナリオ3b (高カーディナリティ)
       │
       └─── 急増→高止まり ──▶ ┌────────────────────────┐
                              │ Queue Usage は？        │
                              └────────────────────────┘
                                     │
                                     ├─── 100% 張り付き ──▶ シナリオ1 (下流停止)
                                     │
                                     └─── 70-90% 乱高下 ──▶ ┌──────────────────────────┐
                                                            │ Refused は恒常的？       │
                                                            └──────────────────────────┘
                                                                   │
                                                                   ├─── Yes ──▶ シナリオ2 (キャパ過多)
                                                                   │
                                                                   └─── No ──▶ 正常動作 または シナリオ3a相当
```

### 診断ポイントまとめ

| 症状 | Queue Usage | スループット | Heap Memory | 原因 |
|------|-------------|-------------|-------------|------|
| **シナリオ1** | 100% 張り付き | 急減 | 急増→急落→高止まり | 下流停止 |
| **シナリオ2** | 70-90% 乱高下 | 安定だが低い | 高位で安定 | キャパシティ不足 |
| **シナリオ3b** | 変動 | **段階的低下** | **右肩上がり** | 高カーディナリティ |
| **シナリオ4** | スパイクで急増（30-50%） | **10秒ごとに変動** | **テントウ虫型（復帰遅い）** | 不適切な batch 設定 |
